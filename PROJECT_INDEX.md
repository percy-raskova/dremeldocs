# DremelDocs Project Index

## ğŸ“š Project Overview

**DremelDocs** is a sophisticated Twitter archive processing pipeline that transforms 21,723 tweets into a curated MkDocs knowledge base focused on philosophical and political content. The system employs advanced NLP techniques to extract, filter, and classify meaningful discourse threads.

### Key Metrics
- **Input**: 37MB Twitter archive (21,723 tweets)
- **Output**: 1,363 filtered threads â†’ 59 heavy hitter documents
- **Completion**: 85% (blocked on manual theme extraction)
- **Test Coverage**: 119 tests, 100% passing
- **Tech Stack**: Python 3.12, SpaCy, MkDocs Material, uv package manager

---

## ğŸ—ï¸ Architecture

### System Components

```mermaid
graph TD
    A[Twitter Archive<br/>37MB tweets.js] -->|Stream Processing| B[Local Filter Pipeline]
    B -->|Thread Detection| C[Filtered Threads<br/>1,363 threads]
    C -->|Length Analysis| D[Heavy Hitters<br/>59 documents]
    D -->|Manual Review| E[Theme Extraction<br/>USER ACTION]
    E -->|Classification| F[Theme Classifier]
    F -->|Generation| G[MkDocs Site<br/>markdown/]
```

### Directory Structure

```
dremeldocs/
â”œâ”€â”€ ğŸ“ config/              # Configuration management
â”‚   â”œâ”€â”€ pipeline.yml        # â†’ Centralized path configuration
â”‚   â”œâ”€â”€ environments/       # â†’ Environment-specific settings
â”‚   â””â”€â”€ loader.py          # â†’ Configuration loader (planned)
â”‚
â”œâ”€â”€ ğŸ“ scripts/             # Core processing pipeline
â”‚   â”œâ”€â”€ local_filter_pipeline.py    # â†’ Entry point: tweet extraction
â”‚   â”œâ”€â”€ generate_themed_markdown.py # â†’ Heavy hitter document generation
â”‚   â”œâ”€â”€ theme_classifier.py        # â†’ Thread classification engine
â”‚   â”œâ”€â”€ run_full_pipeline.py       # â†’ Orchestration script
â”‚   â””â”€â”€ archived_experiments/      # â†’ Historical implementations
â”‚
â”œâ”€â”€ ğŸ“ data/               # Processing artifacts
â”‚   â”œâ”€â”€ filtered_threads.json      # â†’ 1,363 extracted threads
â”‚   â”œâ”€â”€ classified_threads.json    # â†’ Theme-tagged threads
â”‚   â””â”€â”€ vocabularies/              # â†’ Domain vocabulary definitions
â”‚
â”œâ”€â”€ ğŸ“ markdown/           # MkDocs source (PRODUCTION)
â”‚   â”œâ”€â”€ index.md                   # â†’ Homepage
â”‚   â”œâ”€â”€ themes/                    # â†’ Revolutionary theory content
â”‚   â”œâ”€â”€ analysis/                  # â†’ Critical analysis section
â”‚   â””â”€â”€ about/                     # â†’ Project documentation
â”‚
â”œâ”€â”€ ğŸ“ docs/               # Staging documentation
â”‚   â””â”€â”€ heavy_hitters/             # â†’ 59 long-form threads for review
â”‚
â”œâ”€â”€ ğŸ“ tests/              # Comprehensive test suite
â”‚   â”œâ”€â”€ unit/                      # â†’ Module-level testing
â”‚   â”œâ”€â”€ integration/               # â†’ End-to-end workflows
â”‚   â””â”€â”€ scripts/                   # â†’ Script-specific tests
â”‚
â””â”€â”€ ğŸ“ templates/          # MkDocs theme customization
```

---

## ğŸ”„ Data Pipeline

### Processing Stages

| Stage | Script | Input | Output | Status |
|-------|--------|-------|--------|--------|
| 1. Extract | `local_filter_pipeline.py` | tweets.js (37MB) | filtered_threads.json | âœ… Complete |
| 2. Analyze | `generate_heavy_hitters.py` | filtered_threads.json | docs/heavy_hitters/*.md | âœ… Complete |
| 3. Review | Manual Process | heavy_hitters/*.md | THEMES_EXTRACTED.md | â³ **BLOCKED** |
| 4. Classify | `theme_classifier.py` | THEMES_EXTRACTED.md | classified_threads.json | â³ Waiting |
| 5. Generate | `theme_classifier.py` | classified_threads.json | markdown/themes/* | â³ Waiting |

### Key Algorithms

#### Thread Detection
- **Method**: Reply chain analysis using `reply_to_status_id`
- **Criteria**: Minimum 2 tweets per thread
- **Heavy Hitters**: Threads with 500+ total words

#### NLP Processing
```python
# Core NLP components (scripts/nlp_core.py)
- EnhancedTagExtractor: Advanced tag extraction with scoring
- ChunkScorer: Noun phrase relevance scoring
- DomainVocabulary: Domain-specific term recognition
- PatternMatcher: Regex-based pattern extraction
```

---

## ğŸ“– API Reference

### Core Modules

#### `scripts/local_filter_pipeline.py`
**Purpose**: Extract and filter tweet threads from Twitter archive

| Function | Description | Parameters | Returns |
|----------|-------------|------------|---------|
| `process_archive()` | Main entry point | `archive_path: str` | `List[Thread]` |
| `detect_threads()` | Find reply chains | `tweets: List[Tweet]` | `List[Thread]` |
| `filter_quality()` | Apply quality filters | `threads: List[Thread]` | `List[Thread]` |

#### `scripts/theme_classifier.py`
**Purpose**: Classify threads and generate themed markdown

| Function | Description | Parameters | Returns |
|----------|-------------|------------|---------|
| `classify_threads()` | Apply theme tags | `threads: List, themes: Dict` | `Dict[str, List]` |
| `generate_markdown()` | Create MD files | `classified: Dict` | `int` (file count) |
| `clear_markdown()` | Clean output dir | `--clear-only` flag | `None` |

#### `scripts/nlp_core.py`
**Purpose**: NLP utilities and text processing

| Class | Description | Key Methods |
|-------|-------------|-------------|
| `EnhancedTagExtractor` | Extract meaningful tags | `extract_tags()`, `score_chunk()` |
| `ChunkScorer` | Score text relevance | `calculate_score()` |
| `DomainVocabulary` | Domain term management | `load_vocabulary()`, `match_terms()` |

---

## ğŸ§ª Test Suite

### Structure
```
tests/
â”œâ”€â”€ unit/                  # Unit tests (98 tests)
â”‚   â”œâ”€â”€ test_text_processing.py
â”‚   â”œâ”€â”€ test_generate_heavy_hitters.py
â”‚   â””â”€â”€ test_frontmatter_generation.py
â”œâ”€â”€ integration/           # Integration tests (21 tests)
â”‚   â”œâ”€â”€ test_filter_pipeline.py
â”‚   â””â”€â”€ test_end_to_end.py
â”œâ”€â”€ fixtures/             # Test data and utilities
â”‚   â””â”€â”€ sample_data.py
â””â”€â”€ conftest.py          # pytest configuration
```

**Coverage**: 89% on core modules
**Status**: 119/119 tests passing (100%)
**Command**: `uv run pytest tests/ --cov=scripts`

---

## ğŸš€ Quick Start

### Prerequisites
```bash
# Install uv package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone repository
git clone <repository-url>
cd dremeldocs

# Install dependencies
uv pip install -e .
./install_spacy_model.sh
```

### Running the Pipeline
```bash
# Full pipeline execution
make pipeline

# Or step-by-step:
uv run python scripts/local_filter_pipeline.py     # Extract threads
uv run python scripts/generate_heavy_hitters.py    # Generate documents
# [USER ACTION: Review and extract themes]
uv run python scripts/theme_classifier.py          # Classify and generate

# Preview site
mkdocs serve  # http://localhost:8000
```

---

## ğŸ¨ Website Structure

### MkDocs Configuration
**Location**: `mkdocs.yml`
**Theme**: Material with custom CSS
**Features**: Dark mode, responsive, metadata cards

### Content Organization
```
markdown/
â”œâ”€â”€ index.md              # Homepage with dedication
â”œâ”€â”€ about/                # AI collaboration page
â”‚   â””â”€â”€ ai-ethics.md     # Leftist perspective on AI
â”œâ”€â”€ themes/               # Organized by classification
â”‚   â”œâ”€â”€ philosophy/       # Philosophical threads
â”‚   â”œâ”€â”€ politics/         # Political analysis
â”‚   â””â”€â”€ science/          # Scientific discussions
â””â”€â”€ stylesheets/          # Custom CSS
    â””â”€â”€ extra.css        # Courier Prime font, earthy colors
```

---

## ğŸ“‹ Development Workflow

### Available Commands
```bash
make help         # Show all available commands
make test         # Run test suite
make lint         # Check code quality
make format       # Auto-format code
make clean        # Clean generated files
make docs-serve   # Serve documentation locally
make docs-build   # Build static site
```

### Code Quality Tools
- **Linter**: Ruff (configured in `.ruff.toml`)
- **Formatter**: Black
- **Type Checker**: mypy
- **Test Runner**: pytest with coverage

---

## ğŸ¯ Current Status

### Completed âœ…
- Pipeline infrastructure (streaming, thread detection, filtering)
- NLP processing components (tag extraction, scoring, vocabularies)
- Heavy hitter document generation (59 documents)
- Test suite implementation (119 tests, 100% passing)
- MkDocs Material theme configuration

### Blocked â³
- **Manual Theme Extraction**: User must review `docs/heavy_hitters/*.md` and complete `THEMES_EXTRACTED.md`
- **Theme Classification**: Waiting on extracted themes
- **Final Site Generation**: Dependent on classification

### Next Actions
1. ğŸ‘¤ **User Action**: Review heavy hitters and extract themes
2. ğŸ¤– Run theme classification: `uv run python scripts/theme_classifier.py`
3. ğŸŒ Deploy site: `mkdocs build && mkdocs gh-deploy`

---

## ğŸ“š Knowledge Base

### Design Decisions
- **Streaming Processing**: ijson for memory-efficient 37MB file handling
- **Two-Stage Filtering**: Length filter â†’ Thread detection for quality
- **Human-in-the-Loop**: Manual theme extraction for better classification
- **Dual Directory Structure**: `markdown/` (production) vs `docs/` (staging)

### Performance Characteristics
| Metric | Value |
|--------|-------|
| Processing Time | ~2 minutes full pipeline |
| Memory Usage | 50MB peak (streaming) |
| Input Size | 37MB (21,723 tweets) |
| Output Size | 4MB filtered JSON |
| Reduction Rate | 96% (21,723 â†’ 1,363) |

### Technical Patterns
```python
# SpaCy model loading (uv-specific)
try:
    nlp = spacy.load("en_core_web_lg")
except OSError:
    print("Install with: uv pip install [URL]")
    sys.exit(1)

# Twitter archive structure
window.YTD.category_name.part0 = [ /* array */ ]

# Thread detection logic
if tweet.reply_to_status_id == parent.id:
    thread.append(tweet)
```

---

## ğŸ”— Cross-References

### Related Documentation
- Technical details â†’ [`ARCHITECTURE.md`](docs/ARCHITECTURE.md)
- API reference â†’ [`API.md`](docs/API.md)
- Current status â†’ [`STATUS.md`](docs/STATUS.md)
- Testing guide â†’ [`testing.md`](docs/testing.md)
- Setup instructions â†’ [`setup.md`](docs/setup.md)

### Key Memories (Serena)
- Complete reference â†’ `PROJECT_KNOWLEDGE_BASE`
- SpaCy/uv insights â†’ `technical_learnings_spacy_uv`
- NLP setup â†’ `project_nlp_infrastructure`
- Quick commands â†’ `suggested_commands`

### Session History
- Development timeline â†’ `.notes/session_history/`
- Cleanup summary â†’ `.notes/CLEANUP_SUMMARY.md`
- Best practices â†’ `.notes/SERENA_MEMORY_BEST_PRACTICES.md`

---

## ğŸš€ Quick Start

```bash
# Clone and setup
cd /home/percy/projects/dremeldocs
uv pip install -r requirements.txt

# Run tests
uv run pytest tests/

# Process data
uv run python scripts/local_filter_pipeline.py
uv run python scripts/generate_heavy_hitters.py

# Review heavy hitters
ls docs/heavy_hitters/*.md

# [MANUAL STEP] Extract themes
# Edit THEME_TEMPLATE.md â†’ THEMES_EXTRACTED.md

# Complete processing
uv run python scripts/theme_classifier.py

# Preview site
mkdocs serve
```

---

## ğŸ“ Notes

- Project is 85% complete, blocked on manual theme extraction
- All infrastructure ready for final processing
- Estimated 1-2 hours user work + 20 minutes automated processing
- Website design complete with romantic dedication to Meredith

---

*Generated: 2025-01-23 | DremelDocs Project Index v1.0*